{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "base_root = '/Users/admin/Documents/PhD/Code/perceptual-tuning-results/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting distinct model representations of a phonetic category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os.path as path\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data pre-processing\n",
    "\n",
    "def get_counts(root, train, test, dur=10, by_spk=True, seed=0, within_exclusion=True):\n",
    "    if by_spk:\n",
    "        spk_str = ''\n",
    "    else:\n",
    "        spk_str = '_multispk'\n",
    "    if not(within_exclusion) and by_spk:\n",
    "        exc_str =  '_noexclusion'\n",
    "    else:\n",
    "        exc_str = ''\n",
    "    template = ('dominant_units_{}ms_around_central_frame_{}'\n",
    "                '_{}_most_conservative{}{}{}.txt')\n",
    "    filename = path.join(root, template.format(dur, train, test, spk_str, seed, exc_str))\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_phonetic_categories(root, test, by_spk=True, within_exclusion=True):\n",
    "    if by_spk:\n",
    "        spk_str = ''\n",
    "    else:\n",
    "        spk_str = 'multispk_'\n",
    "    if not(within_exclusion) and by_spk:\n",
    "        exc_str =  '_noexclusion'\n",
    "    else:\n",
    "        exc_str = ''\n",
    "    template = 'contextphones_most_conservative_{}{}{}_phoncat_{}.txt'\n",
    "    filename = path.join(root, template.format(spk_str, test, exc_str, 'types'))\n",
    "    types = pd.read_csv(filename, index_col=0)\n",
    "    filename = path.join(root, template.format(spk_str, test, exc_str, 'items'))\n",
    "    items = pd.read_csv(filename, index_col=0)\n",
    "    return types, items\n",
    "\n",
    "\n",
    "def augment_counts(counts, types, **kwargs):\n",
    "    # add various columns from types\n",
    "    cols = [e for e in types.columns if e!='size']\n",
    "    for col in cols:\n",
    "        counts[col] = [types[col].loc[id] for id in counts['context+phone ID']]\n",
    "    # add lb-ub column\n",
    "    cols = counts.columns\n",
    "    ub_cols = [e for e in counts.columns if e != 'nunique_lb']\n",
    "    ub_df = counts[ub_cols]\n",
    "    ub_df['count type'] = 'upper bound'\n",
    "    del ub_df['nunique']\n",
    "    ub_df = ub_df.rename(columns={'nunique_ub': 'nunique'})\n",
    "    lb_cols = [e for e in counts.columns if e != 'nunique_ub']\n",
    "    lb_df = counts[lb_cols]\n",
    "    lb_df['count type'] = 'lower bound'\n",
    "    del lb_df['nunique']\n",
    "    lb_df = lb_df.rename(columns={'nunique_lb': 'nunique'})\n",
    "    counts = pd.concat([ub_df, lb_df])\n",
    "    # add other regressors\n",
    "    for arg_name in kwargs:\n",
    "        counts[arg_name] = kwargs[arg_name]\n",
    "    return counts\n",
    "\n",
    "\n",
    "def prepare_data(root, train_corpora, test_corpora, by_spks, durs, seeds, within_exclusion=True):\n",
    "    # main loading function\n",
    "    all_counts = []\n",
    "    for train, test in zip(train_corpora, test_corpora):\n",
    "        print(train)\n",
    "        print(test)\n",
    "        for by_spk in by_spks:\n",
    "            print(by_spk)\n",
    "            types, items = get_phonetic_categories(root, test, by_spk=by_spk, within_exclusion=within_exclusion) \n",
    "            for dur in durs:\n",
    "                for seed in seeds:\n",
    "                    counts = get_counts(root, train, test, by_spk=by_spk, dur=dur, seed=seed, within_exclusion=within_exclusion)\n",
    "                    kwargs = {'train': train, 'test': test, 'by_spk': by_spk, 'dur': dur, 'seed': seed}\n",
    "                    if not(by_spk):\n",
    "                        kwargs['spk'] = None\n",
    "                    counts = augment_counts(counts, types, **kwargs)\n",
    "                    all_counts.append(counts)\n",
    "    counts = pd.concat(all_counts)\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# various groupings\n",
    "# **Possible extensions: word of a same family, plurals etc.?**\n",
    "\n",
    "def agg_even_words(counts):\n",
    "    \"\"\" \n",
    "    Aggregate on both central phones in words of even length    \n",
    "    \"\"\"\n",
    "    group_cols = ['test', 'train', 'by_spk', 'spk', 'dur', 'seed', 'model', 'count type', 'word_trans']\n",
    "    counts = counts.groupby(group_cols, as_index=False)['nunique'].mean()\n",
    "    return counts\n",
    "\n",
    "\n",
    "def agg_multispk(counts):\n",
    "    # For by_spk row only: aggregate over all available speakers for each word type\n",
    "    by_spk_ix = counts['by_spk']\n",
    "    other_ix = [not(e) for e in counts['by_spk']]\n",
    "    group_cols = ['test', 'train', 'by_spk', 'dur', 'seed', 'model', 'count type', 'word_trans']\n",
    "    counts_by_spk = counts[by_spk_ix].groupby(group_cols, as_index=False)['nunique'].mean()\n",
    "    counts_by_spk['spk'] = 'Aggregated'\n",
    "    counts = pd.concat([counts_by_spk, counts[other_ix]])\n",
    "    del counts['spk']\n",
    "    return counts\n",
    "\n",
    "\n",
    "def agg_seeds(counts):\n",
    "    group_cols = ['test', 'train', 'by_spk', 'dur','model', 'count type', 'word_trans']\n",
    "    count_means = counts.groupby(group_cols, as_index=False)['nunique'].mean()\n",
    "    # for std there is some bug with using as_index=False, so we use reset_index instead\n",
    "    count_stds = counts.groupby(group_cols)['nunique'].std().reset_index()\n",
    "    return count_means, count_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = base_root + 'no_phon_cats/phone_rep_count'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count unique representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pre-process data (done only once)\n",
    "AE_corpora = ['WSJ', 'BUC']\n",
    "JP_corpora = ['GPJ', 'CSJ']\n",
    "corp_AE = [(train_corpus, test_corpus) for train_corpus in AE_corpora for test_corpus in ['WSJ']]\n",
    "corp_JP = [(train_corpus, test_corpus) for train_corpus in JP_corpora for test_corpus in ['GPJ']]\n",
    "train_corpora, test_corpora = zip(*(corp_AE+corp_JP))\n",
    "durs = [0,46]  #10*np.arange(1, 6)\n",
    "seeds = np.arange(10)\n",
    "by_spks = [True, False]\n",
    "counts = prepare_data(root, train_corpora, test_corpora, by_spks, durs, seeds, within_exclusion=True)\n",
    "counts.to_csv(path.join(root, 'all_counts_clean_within_spk_stims.txt'))\n",
    "\n",
    "counts = prepare_data(root, train_corpora, test_corpora, by_spks, durs, seeds, within_exclusion=False)\n",
    "counts.to_csv(path.join(root, 'all_counts_no_stim_cleaning.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load preprocessed data\n",
    "\n",
    "def load_data(res_file):\n",
    "    counts = pd.read_csv(res_file, index_col=0, low_memory=False)\n",
    "    assert not('None' in counts['spk'])\n",
    "    counts['spk'] = [spk if by_spk else \"None\" for by_spk, spk in zip(counts['by_spk'], counts['spk'])]\n",
    "    return counts\n",
    "\n",
    "\n",
    "res_file_within_clean = path.join(root, 'all_counts_clean_within_spk_stims.txt')\n",
    "res_file_no_clean = path.join(root, 'all_counts_no_stim_cleaning.txt')\n",
    "counts_within_clean = load_data(res_file_within_clean)\n",
    "counts = load_data(res_file_no_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Do some structured averaging\n",
    "def process_counts(counts):\n",
    "    counts = agg_even_words(counts)\n",
    "    counts = agg_multispk(counts)\n",
    "    count_means, count_stds = agg_seeds(counts)\n",
    "    return count_means, count_stds\n",
    "\n",
    "#count_means, count_stds = process_counts(counts_within_clean)\n",
    "count_means, count_stds = process_counts(counts)\n",
    "count_means = count_means[((count_means['count type'] == 'upper bound') & (count_means['model'] != 'GMM'))\n",
    "                         |((count_means['count type'] == 'lower bound') & (count_means['model'] == 'GMM'))]\n",
    "\n",
    "count_means.to_csv(base_root + 'no_phon_cats/results/nb_unq.txt')\n",
    "\n",
    "\n",
    "#count_means, count_stds = process_counts(counts_within_clean)\n",
    "count_means, count_stds = process_counts(counts_within_clean)\n",
    "count_means = count_means[((count_means['count type'] == 'upper bound') & (count_means['model'] != 'GMM'))\n",
    "                         |((count_means['count type'] == 'lower bound') & (count_means['model'] == 'GMM'))]\n",
    "\n",
    "count_means.to_csv(base_root + 'no_phon_cats/results/nb_unq_within_spk_cleaned.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Count unique representations for 1-2h models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = base_root + 'no_phon_cats_1h/phone_rep_count'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# pre-process data (done only once)\n",
    "AE_corpora = ['WSJ', 'BUC']\n",
    "JP_corpora = ['GPJ', 'CSJ']\n",
    "for subcorpus in range(1,11):\n",
    "    corp_AE = [(\"{}_{}\".format(train_corpus, subcorpus), test_corpus) for train_corpus in AE_corpora for test_corpus in ['WSJ']]\n",
    "    corp_JP = [(\"{}_{}\".format(train_corpus, subcorpus), test_corpus) for train_corpus in JP_corpora for test_corpus in ['GPJ']]\n",
    "    train_corpora, test_corpora = zip(*(corp_AE+corp_JP))\n",
    "    durs = [0, 46]  #10*np.arange(1, 6)\n",
    "    seeds = np.arange(10)\n",
    "    by_spks = [True, False]\n",
    "    \n",
    "    counts = prepare_data(root, train_corpora, test_corpora, by_spks, durs, seeds, within_exclusion=True)\n",
    "    counts.to_csv(path.join(root, 'all_counts_clean_within_spk_stims_subcorpus{}.txt'.format(subcorpus)))\n",
    "\n",
    "    counts = prepare_data(root, train_corpora, test_corpora, by_spks, durs, seeds, within_exclusion=False)\n",
    "    counts.to_csv(path.join(root, 'all_counts_no_stim_cleaning_subcorpus{}.txt'.format(subcorpus)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load preprocessed data\n",
    "\n",
    "def load_data(res_file):\n",
    "    counts = pd.read_csv(res_file, index_col=0, low_memory=False)\n",
    "    assert not('None' in counts['spk'])\n",
    "    counts['spk'] = [spk if by_spk else \"None\" for by_spk, spk in zip(counts['by_spk'], counts['spk'])]\n",
    "    return counts\n",
    "\n",
    "\n",
    "fnames_within_clean = [path.join(root, 'all_counts_clean_within_spk_stims_subcorpus{}.txt'.format(subcorp)) \n",
    "                       for subcorp in range(1,11)]\n",
    "fnames_no_clean = [path.join(root, 'all_counts_no_stim_cleaning_subcorpus{}.txt'.format(subcorp))\n",
    "                   for subcorp in range(1,11)]\n",
    "counts_within_clean = {i: load_data(fnames_within_clean[i-1]) for i in range(1,11)}\n",
    "counts = {i: load_data(fnames_no_clean[i-1]) for i in range(1,11)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Do some structured averaging\n",
    "def process_counts(counts):\n",
    "    counts = agg_even_words(counts)\n",
    "    counts = agg_multispk(counts)\n",
    "    count_means, count_stds = agg_seeds(counts)\n",
    "    return count_means, count_stds\n",
    "\n",
    "#count_means, count_stds = process_counts(counts_within_clean)\n",
    "# we ignore the count deviations due to random seeds here, as we are not interested conceptually in thi source of noise\n",
    "count_means = {i: process_counts(counts[i])[0] for i in range(1,11)}\n",
    "# we take only the lower bounds since all model considered are GMM and we want to be conservative\n",
    "for i in range(1,11):\n",
    "    count_means[i] = count_means[i][(count_means[i]['count type'] == 'lower bound')]\n",
    "    count_means[i].to_csv(base_root + 'no_phon_cats_1h/results/nb_unq_{}.txt'.format(i))\n",
    "\n",
    "\n",
    "#count_means, count_stds = process_counts(counts_within_clean)\n",
    "# we ignore the count deviations due to random seeds here, as we are not interested conceptually in thi source of noise\n",
    "count_means = {i: process_counts(counts_within_clean[i])[0] for i in range(1,11)}\n",
    "# we take only the lower bounds since all model considered are GMM and we want to be conservative\n",
    "for i in range(1,11):\n",
    "    count_means[i] = count_means[i][(count_means[i]['count type'] == 'lower bound')]\n",
    "    count_means[i].to_csv(base_root + 'no_phon_cats_1h/results/nb_unq_within_spk_cleaned_{}.txt'.format(i))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
