{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = '/Users/admin/Documents/PhD/Code/perceptual-tuning-results/'\n",
    "\n",
    "mp_folder = root + 'ABX/mp_scores'\n",
    "\n",
    "analysis_folder = root + 'ABX/analyses/RL_AmEnglish/resampling'\n",
    "\n",
    "fig_path = root + 'ABX/figures/rl_input_amount.pdf'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amount of input operationalised as total duration in seconds. See notebook for data preparation in perceptual-tuning-pnas/data git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Uncomment for development/debugging\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Uncomment to plot finalized figures\n",
    "\"\"\"\n",
    "import matplotlib as mpl\n",
    "mpl.use(\"pgf\")\n",
    "pgf_with_custom_preamble = {\n",
    "    \"font.family\": \"serif\", # use serif/main font for text elements\n",
    "    \"text.usetex\": True,    # use inline math for ticks\n",
    "    \"pgf.rcfonts\": False,   # don't setup fonts from rc parameters\n",
    "    \"pgf.preamble\": [\n",
    "         \"\\\\usepackage{unicode-math}\",  # unicode math setup\n",
    "         \"\\\\setmainfont{Doulos SIL}\" # serif font via preamble\n",
    "         ]\n",
    "}\n",
    "mpl.rcParams.update(pgf_with_custom_preamble)\n",
    "\"\"\"\n",
    "from scone_phobia import apply_analysis\n",
    "from scone_phobia.utils.mp_scores import estimate_std\n",
    "from scone_phobia.analyses.avg_error import avg_error\n",
    "import scone_phobia.metadata.add_metadata as add_metadata\n",
    "from scone_phobia.analyses.RL_AmEnglish import RL_AmEnglish as AE_RL\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "import math\n",
    "import scipy.special"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading subcorpus data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# avg_error analysis results (without resampling)\n",
    "\n",
    "# select relevant models\n",
    "dpgmm = 'dpgmm_novtln_corpus'\n",
    "percTun_filt = lambda mp_fname: dpgmm in mp_fname\n",
    "\n",
    "# launch analyses\n",
    "analysis = avg_error\n",
    "\n",
    "df_avg = apply_analysis(analysis, mp_folder,\n",
    "                        filt=percTun_filt,\n",
    "                        add_metadata=add_metadata.language_register,\n",
    "                        resampling=False,\n",
    "                        verbose=2)\n",
    "\n",
    "# For RL we only need mp_files tested on AE\n",
    "RL_filt = lambda mp_fname: percTun_filt(mp_fname) and \\\n",
    "                          ('BUCtest' in mp_fname or 'WSJtest' in mp_fname)\n",
    "analysis = AE_RL\n",
    "# get data\n",
    "df_rl = apply_analysis(analysis, mp_folder,\n",
    "                       filt=RL_filt,\n",
    "                       add_metadata=add_metadata.language_register,\n",
    "                       resampling=False,\n",
    "                       verbose=2)\n",
    "\n",
    "# Concat two dfs\n",
    "df_rl['contrast type'] = df_rl['contrast']\n",
    "del df_rl['contrast']\n",
    "df = pd.concat([df_avg, df_rl])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parsing number of sampling iteration, subcorpus size and id from model name and concatenating\n",
    "def identify_subcorpus(df):\n",
    "    s_N, s_id, iter_id = [], [], [] \n",
    "    for model in df['model type']:\n",
    "        tokens = model.split('_')\n",
    "        assert len(tokens) == 4\n",
    "        nb_subsets, subset_id = tokens[2][6:].split('-')\n",
    "        nb_subsets, subset_id = int(nb_subsets), int(subset_id)\n",
    "        iteration = tokens[3][4:]\n",
    "        if '-final' in iteration:\n",
    "            iteration = iteration[:-6]\n",
    "        iteration = int(iteration)\n",
    "        s_N.append(nb_subsets)\n",
    "        s_id.append(subset_id)\n",
    "        iter_id.append(iteration)\n",
    "    df['Number of sampling iterations'] = iter_id\n",
    "    df['Training set splitting factor'] = s_N\n",
    "    df['Training subcorpus'] = s_id\n",
    "    del df['model type']\n",
    "    return df\n",
    "\n",
    "# identify subcorpora to be linked by a line\n",
    "def subcorpus_group(e):\n",
    "    if e>100:\n",
    "        r = e//100+1\n",
    "    elif e>10:\n",
    "        r = e//10+1\n",
    "    else:\n",
    "        r=e\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = identify_subcorpus(df)\n",
    "df[\"Subcorpus group\"] = [subcorpus_group(e) for e in df[\"Training subcorpus\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add scores for full corpora\n",
    "\n",
    "# select relevant models\n",
    "dpgmm = 'dpgmm_novtln_vad'\n",
    "percTun_filt = lambda mp_fname: dpgmm in mp_fname\n",
    "\n",
    "# launch analyses\n",
    "analysis = avg_error\n",
    "\n",
    "df_avg_fullcorpus = apply_analysis(analysis, mp_folder,\n",
    "                                   filt=percTun_filt,\n",
    "                                   add_metadata=add_metadata.language_register,\n",
    "                                   resampling=False,\n",
    "                                   verbose=2)\n",
    "\n",
    "analysis = AE_RL\n",
    "# get data\n",
    "df_rl_fullcorpus = apply_analysis(analysis, mp_folder,\n",
    "                                  filt=RL_filt,\n",
    "                                  add_metadata=add_metadata.language_register,\n",
    "                                  resampling=False,\n",
    "                                  verbose=2)\n",
    "\n",
    "# Concat two dfs\n",
    "df_rl_fullcorpus['contrast type'] = df_rl_fullcorpus['contrast']\n",
    "del df_rl_fullcorpus['contrast']\n",
    "df_fullcorpus = pd.concat([df_avg_fullcorpus, df_rl_fullcorpus])\n",
    "df_fullcorpus[\"Subcorpus group\"] = [1]*len(df_fullcorpus)\n",
    "df_fullcorpus[\"Training set splitting factor\"] = [1]*len(df_fullcorpus)\n",
    "df_fullcorpus[\"Number of sampling iterations\"] = [1501]*len(df_fullcorpus)\n",
    "df = pd.concat([df, df_fullcorpus])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at effect trajectories as a function of training data amount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# selecting relevant data subset\n",
    "data = pd.concat([df[(df['Number of sampling iterations'] == 1501)]])\n",
    "\n",
    "# mapping splitting factors to data amount\n",
    "training_amount = {'WSJ': 19*3600+30*60, 'GPJ': 19*3600+33*60, \n",
    "                   'BUC': 9*3600+13*60, 'CSJ': 9*3600+11*60}  # rounded to the minute\n",
    "data['training amount'] = [training_amount[training_set]/float(factor)\n",
    "                               for training_set, factor in zip(data['training set'], data['Training set splitting factor'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Main figure\n",
    "facet_labels = ['Read test stimuli\\nAmerican English [\\\\textipa{\\*r}]-[l]',\n",
    "                'Read test stimuli\\nAmerican English [w]-[j]',\n",
    "                'Spont. test stimuli\\nAmerican English [\\\\textipa{\\*r}]-[l]',\n",
    "                'Spont. test stimuli\\nAmerican English [w]-[j]']\n",
    "\n",
    "colors = [\"red\", \"red\", \"blue\", \"blue\"]\n",
    "palette = sns.xkcd_palette(colors)\n",
    "\n",
    "g = sns.relplot(x=\"training amount\", y=\"error\", kind=\"line\",\n",
    "                hue=\"training set\",\n",
    "                hue_order=['WSJ', 'BUC', 'GPJ', 'CSJ'],\n",
    "                size=\"training set\",\n",
    "                size_order=['WSJ', 'BUC', 'GPJ', 'CSJ'],\n",
    "                sizes=[1, 1, 1, 1],\n",
    "                style=\"training set\",\n",
    "                style_order=['WSJ', 'BUC', 'GPJ', 'CSJ'],\n",
    "                dashes=[(9999,1), (10,3), (9999,1), (10,3)],\n",
    "                row=\"test set\",\n",
    "                row_order=[\"WSJ\", \"BUC\"],\n",
    "                col=\"contrast type\",\n",
    "                col_order=['L-R', 'W-Y'],\n",
    "                markers=['s', 'D', 's', 'D'],\n",
    "                markersize=10,\n",
    "                #units=\"Subcorpus group\", estimator=None,\n",
    "                data=data,\n",
    "                legend=False,\n",
    "                palette=palette)\n",
    "\n",
    "\n",
    "g = g.set(xscale=\"log\")\n",
    "for ax in g.axes.flatten():\n",
    "    ax.grid()\n",
    "g.set(xticks=[60, 600, 3600, 36000])\n",
    "g.set_xticklabels(['1min', '10min', '1h', '10h'], fontsize=15)\n",
    "for axes in g.axes:\n",
    "    for tick in axes[0].yaxis.get_major_ticks():\n",
    "        tick.label.set_fontsize(20)\n",
    "g.set_ylabels('ABX error rate (in \\%)', fontsize=20)\n",
    "g.set_xlabels('Training set size', fontsize=20)\n",
    "for ax, t in zip(g.axes.flatten(), facet_labels):\n",
    "    ax.tick_params(axis='both', which='both', width=0, length=0)\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.set_title(t, fontsize=20)\n",
    "#g.despine(left=True, bottom=True)\n",
    "# y range set to half that for fig2?\n",
    "g.axes[0,0].set_ylim([0, 33])\n",
    "g.fig.tight_layout()\n",
    "\n",
    "\n",
    "g.savefig(fig_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
